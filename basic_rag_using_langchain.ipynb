{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('‚Ä¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY='your api key'\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-pro',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(model_name = \"gemini-pro\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is an ai agent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> An AI agent is a computer system that is able to perceive its environment and take actions to achieve its goals. AI agents are often used in robotics, gaming, and other applications where it is necessary for the computer to be able to make decisions and act autonomously.\n",
       "> \n",
       "> There are many different types of AI agents, but they all share some common characteristics. First, AI agents must be able to perceive their environment. This can be done through sensors, cameras, or other input devices. Second, AI agents must be able to reason about their environment and make decisions. This can be done using a variety of techniques, such as machine learning, logic, and probability theory. Finally, AI agents must be able to act on their decisions. This can be done through motors, actuators, or other output devices.\n",
       "> \n",
       "> AI agents are becoming increasingly sophisticated, and they are being used in a wider range of applications. As AI technology continues to develop, AI agents are likely to play an increasingly important role in our lives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What are the usecases of LLMs?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> **Text Generation:**\n",
       "> * Content creation (e.g., articles, scripts, poems)\n",
       "> * Summarization and paraphrasing\n",
       "> * Chatbots and virtual assistants\n",
       "> * Language translation\n",
       "> \n",
       "> **Text Analysis:**\n",
       "> * Sentiment analysis\n",
       "> * Question answering\n",
       "> * Text classification\n",
       "> * Code generation\n",
       "> \n",
       "> **Customer Service and Support:**\n",
       "> * Automated customer service chatbots\n",
       "> * Help document generation and search\n",
       "> * Ticket routing and resolution\n",
       "> \n",
       "> **Business Intelligence:**\n",
       "> * Market research and analysis\n",
       "> * Competitive intelligence\n",
       "> * Financial forecasting\n",
       "> * Risk assessment\n",
       "> \n",
       "> **Healthcare:**\n",
       "> * Medical diagnosis and treatment planning\n",
       "> * Drug discovery and research\n",
       "> * Patient record analysis\n",
       "> \n",
       "> **Education:**\n",
       "> * Personalized learning plans\n",
       "> * Intelligent tutoring systems\n",
       "> * Essay grading and feedback\n",
       "> \n",
       "> **Entertainment:**\n",
       "> * Video game storytelling and dialogue\n",
       "> * Music and poetry generation\n",
       "> * Virtual reality and augmented reality experiences\n",
       "> \n",
       "> **Research:**\n",
       "> * Scientific paper generation and analysis\n",
       "> * Data mining and exploration\n",
       "> * Hypothesis testing and model building\n",
       "> \n",
       "> **Other:**\n",
       "> * Code analysis and debugging\n",
       "> * Legal document review\n",
       "> * Social media monitoring and analysis\n",
       "> * Spam and phishing detection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# restart python kernal if issues with langchain import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY,\n",
    "                             temperature=0.2,convert_system_message_to_human=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Moderation Aegis Toxicchat RTP_LX XSafety\n",
      "En Mul En Mul En Mul En Mul En Mul\n",
      "Aegis-Defensive 66.75 56.40 84.95 78.95 63.83 43.93 86.89 86.59 67.61 73.12\n",
      "MD-Judge 76.8 67.03 84.62 35.30 81.05 47.40 92.13 43.14 58.62 28.54\n",
      "LlaMa-Guard-2 75.88 73.54 59.81 55.91 42.14 33.56 40.33 35.80 35.80 32.93\n",
      "LlaMa-Guard-3 78.21 74.23 68.82 63.66 46.29 42.17 49.4 45.30 42.65 39.38\n",
      "Table 2: Benchmarking the performance of guardrails on our test suite. Here we report the F1 score for classifying\n",
      "user prompt safety. En and Mul denotes the performance of English and the non-English, respectively. Number in\n",
      "bold and underline highlights the best and the second-best performance across different models, respectively.\n",
      "handling multilingual harmful inputs. Additionally,\n",
      "although MD-Judge has a better performance on\n",
      "English across different datasets, its performance\n",
      "on multilingual inputs is low. Also, for the XSafety\n",
      "dataset, we observe that the Aegis-Defensive model\n",
      "performs better on non-English data compared to\n",
      "English data. As illustrated in Figure 3, the model\n",
      "exhibits a high False Positive Rate (FPR) on low-\n",
      "resource languages, suggesting a tendency to overly\n",
      "misclassify non-English prompts as unsafe inputs.\n",
      "Since the XSafety dataset exclusively contains in-\n",
      "puts with safety issues, this may explain the ob-\n",
      "served inconsistency in performance.\n",
      "Figure 2: F1 score of different models on Aegis dataset\n",
      "across different languages.\n",
      "To answer question 2, we investigate the poten-\n",
      "tial of using guardrails to detect the multilingual\n",
      "jailbreaking prompts. We used the the existing mul-\n",
      "tilingual jailbreaking dataset, MultiJail (Deng et al.,\n",
      "2023), and additionally evaluate extension of Mul-\n",
      "tijail based on code-switching (CSRT) as proposed\n",
      "in (Yoo et al., 2024). As shown in Table 3 and Fig-\n",
      "ure 4, we observe that the code-switching prompts\n",
      "causes significant drop on guardrails‚Äô performance.\n",
      "MultiJail (Deng et al., 2023) discusses the inten-\n",
      "tional jailbreaking attacks which prepend a mali-\n",
      "cious English instruction to the multilingual harm-\n",
      "ful prompts in MultiJail and shows that this inten-\n",
      "tional attacks further increase the attack success\n",
      "Figure 3: False Positive Rate of different models on\n",
      "Aegis dataset across different languages.\n",
      "F1 Score En Mul CSRT\n",
      "Aegis-Defensive 94.47 83.76 86.28\n",
      "MD-Judge 92.31 37.05 49.64\n",
      "LlaMa-Guard-2 75.25 62.66 62.75\n",
      "LlaMa-Guard-3 80.23 76.70 75.25\n",
      "Table 3: The performance of guardrails on multilingual\n",
      "jailbreaking prompts including their code-switching\n",
      "variants. En denotes the English jailbreaking prompts,\n",
      "Mul denotes the non-English prompts, CSRT denotes\n",
      "the code-switching jailbreaking prompts.\n",
      "Figure 4: F1 Score of different models on Multijail\n",
      "dataset across different languages.\n",
      "rate. As shown in Table 4, the results indicate\n",
      "that two guardrails nearly perfect categorize the\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(r'C:\\Users\\mukht\\Downloads\\2410.22153v1.pdf')\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[3].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "context = \"\\n\\n\".join(str(p.page_content) for p in pages)\n",
    "texts = text_splitter.split_text(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Chroma.from_texts(texts, embeddings).as_retriever(search_kwargs={\"k\":5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    model,\n",
    "    retriever=vector_index,\n",
    "    return_source_documents=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Guardrails are standalone models that can be used to detect and defend against toxic content. They are typically pre-trained on a large dataset of harmful content and can be used to identify and flag potentially harmful content in real-time.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is guardrail\"\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Guardrails are standalone models that can be used to detect and defend against toxic content. They are typically pre-trained on a large dataset of harmful content and can be used to identify and flag potentially harmful content in real-time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891f895bb601417f8dc17003c1b93ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h1 style='text-align: center; color: #4B0082;'>üìö RAG-based Document Q&A Interface<‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .widget-label {\n",
       "        font-weight: bold;\n",
       "        font-size: 14px;\n",
       "    }\n",
       "    .output_wrapper, .output {\n",
       "        border: 1px solid #ddd;\n",
       "        padding: 10px;\n",
       "        border-radius: 4px;\n",
       "        background-color: #f9f9f9;\n",
       "    }\n",
       "    h1, h3 {\n",
       "        font-family: 'Arial', sans-serif;\n",
       "    }\n",
       "    button {\n",
       "        font-size: 14px;\n",
       "    }\n",
       "    .widget-upload {\n",
       "        border: 2px dashed #4B0082;\n",
       "        padding: 20px;\n",
       "        border-radius: 10px;\n",
       "        background-color: #f0e6ff;\n",
       "    }\n",
       "    .widget-text {\n",
       "        border: 2px solid #4B0082;\n",
       "        border-radius: 5px;\n",
       "        padding: 5px;\n",
       "    }\n",
       "    .btn-success {\n",
       "        background-color: #28a745;\n",
       "        color: white;\n",
       "    }\n",
       "    .btn-info {\n",
       "        background-color: #17a2b8;\n",
       "        color: white;\n",
       "    }\n",
       "    .widget-checkbox {\n",
       "        font-size: 14px;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If needed, install dependencies\n",
    "# !pip install google-generativeai langchain chromadb tiktoken pypdf docx2txt openpyxl langchain-google-genai ipywidgets\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown, HTML\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import docx2txt\n",
    "import openpyxl\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# Configuration\n",
    "# -------------------------\n",
    "\n",
    "GOOGLE_API_KEY = 'your api key'  # Replace with your actual API key\n",
    "if GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY\":\n",
    "    raise ValueError(\"Please replace 'YOUR_GOOGLE_API_KEY' with your actual Google API key.\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "MODEL_NAME = \"gemini-pro\"\n",
    "EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "CHUNK_SIZE = 10000\n",
    "CHUNK_OVERLAP = 1000\n",
    "\n",
    "# -------------------------\n",
    "# Helper Functions\n",
    "# -------------------------\n",
    "\n",
    "def load_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    text = \"\\n\\n\".join(p.page_content for p in pages)\n",
    "    return text, len(pages)\n",
    "\n",
    "def load_word(file_path):\n",
    "    text = docx2txt.process(file_path)\n",
    "    return text, text.count(\"\\n\") + 1\n",
    "\n",
    "def load_excel(file_path):\n",
    "    wb = openpyxl.load_workbook(file_path, data_only=True)\n",
    "    all_text = []\n",
    "    for sheet in wb.sheetnames:\n",
    "        ws = wb[sheet]\n",
    "        for row in ws.iter_rows(values_only=True):\n",
    "            row_text = [str(cell) for cell in row if cell is not None]\n",
    "            if row_text:\n",
    "                all_text.append(\" \".join(row_text))\n",
    "    text = \"\\n\".join(all_text)\n",
    "    return text, text.count(\"\\n\") + 1\n",
    "\n",
    "def process_files(uploaded_files):\n",
    "    combined_text = \"\"\n",
    "    doc_info_list = []\n",
    "    for uploaded_file in uploaded_files:\n",
    "        filename = uploaded_file['metadata']['name']\n",
    "        suffix = filename.split(\".\")[-1].lower()\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".\"+suffix) as temp_file:\n",
    "            temp_file.write(uploaded_file['content'])\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            if suffix == \"pdf\":\n",
    "                extracted_text, pages_count = load_pdf(temp_file_path)\n",
    "                doc_info_list.append({\"name\": filename, \"type\": \"PDF\", \"length\": len(extracted_text)})\n",
    "            elif suffix in [\"doc\", \"docx\"]:\n",
    "                extracted_text, line_count = load_word(temp_file_path)\n",
    "                doc_info_list.append({\"name\": filename, \"type\": \"Word\", \"length\": len(extracted_text)})\n",
    "            elif suffix in [\"xls\", \"xlsx\"]:\n",
    "                extracted_text, line_count = load_excel(temp_file_path)\n",
    "                doc_info_list.append({\"name\": filename, \"type\": \"Excel\", \"length\": len(extracted_text)})\n",
    "            else:\n",
    "                extracted_text = \"\"\n",
    "                doc_info_list.append({\"name\": filename, \"type\": \"Unknown\", \"length\": 0})\n",
    "\n",
    "            combined_text += extracted_text + \"\\n\\n\"\n",
    "        except Exception as e:\n",
    "            doc_info_list.append({\"name\": filename, \"type\": \"Error\", \"length\": 0})\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "        finally:\n",
    "            os.remove(temp_file_path)\n",
    "    return combined_text, doc_info_list\n",
    "\n",
    "def build_retriever(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    texts = text_splitter.split_text(text)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=GOOGLE_API_KEY)\n",
    "    vector_index = Chroma.from_texts(texts, embeddings)\n",
    "    retriever = vector_index.as_retriever(search_kwargs={\"k\":5})\n",
    "    return retriever\n",
    "\n",
    "def build_qa_chain(retriever):\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model=MODEL_NAME,\n",
    "        google_api_key=GOOGLE_API_KEY,\n",
    "        temperature=0.2,\n",
    "        convert_system_message_to_human=True\n",
    "    )\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",  # explicitly specify chain type\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "qa_chain = None\n",
    "doc_info = []\n",
    "combined_text = \"\"\n",
    "history = []\n",
    "\n",
    "# -------------------------\n",
    "# Define Widgets\n",
    "# -------------------------\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.pdf,.doc,.docx,.xls,.xlsx',  \n",
    "    multiple=True,\n",
    "    description='üìÑ Upload Files',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='‚úÖ Process Documents',\n",
    "    button_style='success',\n",
    "    tooltip='Click to process uploaded documents',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "question_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your question here',\n",
    "    description='‚ùì Your Question:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "ask_button = widgets.Button(\n",
    "    description='üí¨ Get Answer',\n",
    "    button_style='info',\n",
    "    tooltip='Click to get answer to your question',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "answer_output = widgets.Output()\n",
    "show_sources_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='üìú Show Source Documents',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "history_output = widgets.Output()\n",
    "\n",
    "def update_history():\n",
    "    with history_output:\n",
    "        clear_output()\n",
    "        if history:\n",
    "            display(Markdown(\"### üóíÔ∏è **Question and Answer History:**\"))\n",
    "            for entry in history:\n",
    "                display(Markdown(f\"**Q:** {entry['question']}\"))\n",
    "                display(Markdown(f\"**A:** {entry['answer']}\"))\n",
    "        else:\n",
    "            display(Markdown(\"**No history yet.**\"))\n",
    "\n",
    "def on_process_button_clicked(b):\n",
    "    global qa_chain, doc_info, combined_text, history\n",
    "    with output:\n",
    "        clear_output()\n",
    "        if not file_upload.value:\n",
    "            display(Markdown(\"**üö´ Please upload at least one document.**\"))\n",
    "            return\n",
    "        display(Markdown(\"**‚åõ Processing documents... Please wait.**\"))\n",
    "        uploaded_files = []\n",
    "        for filename, fileinfo in file_upload.value.items():\n",
    "            uploaded_files.append({\n",
    "                'metadata': {'name': filename},\n",
    "                'content': fileinfo['content']\n",
    "            })\n",
    "        combined_text, doc_info = process_files(uploaded_files)\n",
    "        if combined_text.strip():\n",
    "            display(Markdown(\"**üîç Building retriever...**\"))\n",
    "            retriever = build_retriever(combined_text)\n",
    "            # Test the retriever:\n",
    "            test_docs = retriever.get_relevant_documents(\"test\")\n",
    "            if not test_docs:\n",
    "                display(Markdown(\"**‚ö†Ô∏è Warning: No documents retrieved for a test query.**\"))\n",
    "            display(Markdown(\"**ü§ñ Building QA chain...**\"))\n",
    "            qa_chain = build_qa_chain(retriever)\n",
    "            display(Markdown(\"**‚úÖ Document processing complete! You can now ask questions.**\"))\n",
    "            display(Markdown(\"### üìë Uploaded Documents:\"))\n",
    "            for info in doc_info:\n",
    "                display(Markdown(f\"- **{info['name']}** (Type: {info['type']}), Extracted length: {info['length']} chars\"))\n",
    "            history = []\n",
    "            update_history()\n",
    "        else:\n",
    "            display(Markdown(\"**‚ö†Ô∏è No text extracted from the provided documents.**\"))\n",
    "\n",
    "def on_ask_button_clicked(b):\n",
    "    global qa_chain, history\n",
    "    with answer_output:\n",
    "        clear_output()\n",
    "        if qa_chain is None:\n",
    "            display(Markdown(\"**üö´ Please upload and process documents first.**\"))\n",
    "            return\n",
    "        question = question_input.value.strip()\n",
    "        if not question:\n",
    "            display(Markdown(\"**‚ö†Ô∏è Please enter a valid question.**\"))\n",
    "            return\n",
    "        display(Markdown(\"**‚åõ Generating answer... Please wait.**\"))\n",
    "        try:\n",
    "            result = qa_chain({\"query\": question})\n",
    "            answer = result[\"result\"]\n",
    "            source_docs = result.get(\"source_documents\", [])\n",
    "            history.append({\"question\": question, \"answer\": answer})\n",
    "            update_history()\n",
    "            display(Markdown(f\"**Q:** {question}\"))\n",
    "            display(Markdown(f\"**A:** {answer}\"))\n",
    "            if show_sources_checkbox.value and source_docs:\n",
    "                display(Markdown(\"**üìú Source Documents:**\"))\n",
    "                for idx, doc in enumerate(source_docs, start=1):\n",
    "                    snippet = doc.page_content[:200].replace('\\n', ' ') + \"...\"\n",
    "                    display(Markdown(f\"{idx}. {snippet}\"))\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"**‚ö†Ô∏è Error generating answer:** {str(e)}\"))\n",
    "\n",
    "process_button.on_click(on_process_button_clicked)\n",
    "ask_button.on_click(on_ask_button_clicked)\n",
    "\n",
    "upload_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>üìÑ Upload Documents</h3>\"),\n",
    "    file_upload,\n",
    "    process_button,\n",
    "    output\n",
    "])\n",
    "\n",
    "question_box = widgets.HBox([\n",
    "    question_input,\n",
    "    ask_button\n",
    "])\n",
    "\n",
    "ask_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>‚ùì Ask a Question</h3>\"),\n",
    "    question_box,\n",
    "    show_sources_checkbox,\n",
    "    answer_output\n",
    "])\n",
    "\n",
    "history_section = widgets.VBox([history_output])\n",
    "\n",
    "app_layout = widgets.VBox([\n",
    "    widgets.HTML(\"<h1 style='text-align: center; color: #4B0082;'>üìö RAG-based Document Q&A Interface</h1>\"),\n",
    "    upload_box,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    ask_box,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    history_section\n",
    "])\n",
    "\n",
    "display(app_layout)\n",
    "\n",
    "styles = \"\"\"\n",
    "<style>\n",
    "    .widget-label {\n",
    "        font-weight: bold;\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    .output_wrapper, .output {\n",
    "        border: 1px solid #ddd;\n",
    "        padding: 10px;\n",
    "        border-radius: 4px;\n",
    "        background-color: #f9f9f9;\n",
    "    }\n",
    "    h1, h3 {\n",
    "        font-family: 'Arial', sans-serif;\n",
    "    }\n",
    "    button {\n",
    "        font-size: 14px;\n",
    "    }\n",
    "    .widget-upload {\n",
    "        border: 2px dashed #4B0082;\n",
    "        padding: 20px;\n",
    "        border-radius: 10px;\n",
    "        background-color: #f0e6ff;\n",
    "    }\n",
    "    .widget-text {\n",
    "        border: 2px solid #4B0082;\n",
    "        border-radius: 5px;\n",
    "        padding: 5px;\n",
    "    }\n",
    "    .btn-success {\n",
    "        background-color: #28a745;\n",
    "        color: white;\n",
    "    }\n",
    "    .btn-info {\n",
    "        background-color: #17a2b8;\n",
    "        color: white;\n",
    "    }\n",
    "    .widget-checkbox {\n",
    "        font-size: 14px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"\n",
    "display(HTML(styles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
